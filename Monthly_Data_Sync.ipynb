{"cells":[{"cell_type":"code","source":["#init workspace\n","import requests\n","import pandas\n","import json\n","import time\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n","from pyspark.sql import SparkSession\n","\n","geography = {\n","    \"Canada\": 1,\n","    \"Newfoundland and Labrador\": 2,\n","    \"Prince Edward Island\": 3,\n","    \"Nova Scotia\": 4,\n","    \"New Brunswick\": 5,\n","    \"Quebec\": 6,\n","    \"Ontario\": 7,\n","    \"Manitoba\": 8,\n","    \"Saskatchewan\": 9,\n","    \"Alberta\": 10,\n","    \"British Columbia\": 11,\n","    \"Yukon\": 12,\n","    \"Northwest Territories including Nunavut\": 13,\n","    \"Northwest Territories\": 14,\n","    \"Nunavut\": 15\n","}\n","\n","trade = {\n","    \"Total exports\": 1,\n","    \"Domestic exports\": 2,\n","    \"Re-exports\": 3,\n","    \"Total imports\": 4\n","}\n","\n","united_states = {\n","    \"Total United States\": 1,\n","    \"Alaska\": 2,\n","    \"Alabama\": 3,\n","    \"Arkansas\": 4,\n","    \"Arizona\": 5,\n","    \"California\": 6,\n","    \"Colorado\": 7,\n","    \"Connecticut\": 8,\n","    \"District of Columbia\": 9,\n","    \"Delaware\": 10,\n","    \"Florida\": 11,\n","    \"Georgia\": 12,\n","    \"Hawaii\": 13,\n","    \"Iowa\": 14,\n","    \"Idaho\": 15,\n","    \"Illinois\": 16,\n","    \"Indiana\": 17,\n","    \"Kansas\": 18,\n","    \"Kentucky\": 19,\n","    \"Louisiana\": 20,\n","    \"Massachusetts\": 21,\n","    \"Maryland\": 22,\n","    \"Maine\": 23,\n","    \"Michigan\": 24,\n","    \"Minnesota\": 25,\n","    \"Missouri\": 26,\n","    \"Mississippi\": 27,\n","    \"Montana\": 28,\n","    \"North Carolina\": 29,\n","    \"North Dakota\": 30,\n","    \"Nebraska\": 31,\n","    \"New Hampshire\": 32,\n","    \"New Jersey\": 33,\n","    \"New Mexico\": 34,\n","    \"Nevada\": 35,\n","    \"New York\": 36,\n","    \"Ohio\": 37,\n","    \"Oklahoma\": 38,\n","    \"Oregon\": 39,\n","    \"Pennsylvania\": 40,\n","    \"Rhode Island\": 41,\n","    \"South Carolina\": 42,\n","    \"South Dakota\": 43,\n","    \"Tennessee\": 44,\n","    \"Texas\": 45,\n","    \"Utah\": 46,\n","    \"Virginia\": 47,\n","    \"Vermont\": 48,\n","    \"Washington, State\": 49,\n","    \"Wisconsin\": 50,\n","    \"West Virginia\": 51,\n","    \"Wyoming\": 52,\n","    \"Other states\": 53\n","}\n","\n","hs_sections = {\n","    \"Total Harmonized System (HS) sections\": 1,\n","    \"I - Live animals and animal products\": 2,\n","    \"II - Vegetable products\": 3,\n","    \"III - Animal or vegetable fats and oils and their cleavage products, prepared edible fats, animal or vegetable waxes\": 4,\n","    \"IV - Prepared foodstuffs, beverages, spirits and vinegar, tobacco and manufactures tobacco substitutes\": 5,\n","    \"V - Mineral products\": 6,\n","    \"VI - Products of the chemical or allied industries\": 7,\n","    \"VII - Plastics and articles thereof, rubber and articles thereof\": 8,\n","    \"VIII - Raw hides and skins, leather, furskins and articles thereof, saddlery and harness, travel goods, handbags and similar containers, articles of animal gut (other than silk-worm gut)\": 9,\n","    \"IX - Wood and articles of wood, wood charcoal, cork and articles of cork, manufactures of straw, of esparto or of other plaiting materials, basketware and wickerwork\": 10,\n","    \"X - Pulp of wood or of other fibrous cellulosic material, recovered (waste and scrap) paper or paperboard\": 11,\n","    \"XI - Textiles and textile articles\": 12,\n","    \"XII - Footwear, headgear, umbrellas, sun umbrellas, walking-sticks, seat-sticks, whips, riding-crops and parts thereof, prepared feathers and articles made therewith, artificial flowers, articles of human hair\": 13,\n","    \"XIII - Articles of stone, plaster, cement, asbestos, mica or similar materials, ceramic products, glass and glassware\": 14,\n","    \"XIV - Natural or cultured pearls, precious or semi-precious stones, precious metals, metals clad with precious metal and articles thereof, imitation jewellery, coin\": 15,\n","    \"XV - Base metals and articles of base metal\": 16,\n","    \"XVI - Machinery and mechanical appliances, electrical equipment, parts thereof, sound recorders and reproducers, television image and sound recorders and reproducers, and parts and accessories of such articles\": 17,\n","    \"XVII - Vehicles, aircraft, vessels and associated transport equipment\": 18,\n","    \"XVIII - Optical, photographic, cinematographic, measuring, checking, precision, medical or surgical instruments and apparatus, clocks and watches, musical instruments, parts and accessories thereof\": 19,\n","    \"XIX - Arms and ammunition, parts and accessories thereof\": 20,\n","    \"XX - Miscellaneous manufactured articles\": 21,\n","    \"XXI - Works of art, collectors' pieces and antiques\": 22\n","}\n","spark = SparkSession.builder.appName(\"bronze_data\").getOrCreate()\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"163e803b-5d8f-434b-a371-76d6436a972b"},{"cell_type":"code","source":["#GET IMPORT DATA\n","\n","import requests\n","import pandas\n","import json\n","import time\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n","\n","url = \"https://www150.statcan.gc.ca/t1/wds/rest/getDataFromCubePidCoordAndLatestNPeriods\"\n","states = range(2,53)\n","hs_categories = range(1,23)\n","id = 1\n","\n","rows = []\n","\n","for state in states:\n","    # print(\"State: \" + str(state))\n","    for hs in hs_categories:\n","        # print(\"HS:\" + str(hs))\n","\n","        body = [{\"productId\": \"12100099\", \"coordinate\":f\"7.4.{state}.{hs}.0.0.0.0.0.0\", \"latestN\":1}]\n","\n","\n","        try:\n","\n","            response = requests.post(url,json=body, timeout=10)\n","\n","            if(response.status_code != 200):\n","                print(response.status_code)\n","                print(\"too many requests/timeout... nap time ! zzzzzz\")\n","                time.sleep(5)\n","                response = requests.post(url,json=body, timeout=10)\n","                print(response.status_code)\n","\n","        except Exception as e:\n","                print(\"too many requests/timeout... nap time ! zzzzzz\")\n","                time.sleep(5)\n","                response = requests.post(url,json=body, timeout=10)\n","                print(response.status_code)\n","\n","        try:\n","            data = response.json()\n","            datapoints = data[0][\"object\"][\"vectorDataPoint\"]\n","            \n","            for dp in datapoints:\n","                rows.append({\n","                    \"TRADE_DATE\": dp[\"refPer\"],\n","                    \"REPORTER_KEY\": 1,\n","                    \"PARTNER_KEY\": state,\n","                    \"HS_CODE\": hs,\n","                    \"FLOW_CODE\": 4,\n","                    \"CURRENCY_KEY\": 1,\n","                    \"VALUE\": dp[\"value\"]\n","                })\n","                id += 1\n","\n","            print(id)\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Failed to parse data for STATE {state}, HS {hs}: {e}. skipping :P\")\n","            continue\n","\n","print(\"Done\") \n","\n","schema = StructType([\n","    StructField(\"TRADE_DATE\", StringType(), True),\n","    StructField(\"REPORTER_KEY\", IntegerType(), True),\n","    StructField(\"PARTNER_KEY\", IntegerType(), True),\n","    StructField(\"HS_CODE\", IntegerType(), True),\n","    StructField(\"FLOW_CODE\", IntegerType(), True),\n","    StructField(\"CURRENCY_KEY\", IntegerType(), True),\n","    StructField(\"VALUE\", FloatType(), True)         \n","])\n","\n","df = spark.createDataFrame(rows, schema=schema)\n","df.write.mode(\"append\").saveAsTable(\"FACT_TRADE\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386e5575-108f-4506-9867-c9cb71a67742"},{"cell_type":"code","source":["#GET DOMESTIC EXPORT DATA\n","\n","import requests\n","import pandas\n","import json\n","import time\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n","\n","url = \"https://www150.statcan.gc.ca/t1/wds/rest/getDataFromCubePidCoordAndLatestNPeriods\"\n","states = range(2,53)\n","hs_categories = range(1,23)\n","id = 1\n","\n","rows = []\n","\n","for state in states:\n","    # print(\"State: \" + str(state))\n","    for hs in hs_categories:\n","        # print(\"HS:\" + str(hs))\n","\n","        body = [{\"productId\": \"12100099\", \"coordinate\":f\"7.2.{state}.{hs}.0.0.0.0.0.0\", \"latestN\":1}]\n","\n","\n","        try:\n","\n","            response = requests.post(url,json=body, timeout=10)\n","\n","            if(response.status_code != 200):\n","                print(response.status_code)\n","                print(\"too many requests/timeout... nap time ! zzzzzz\")\n","                time.sleep(5)\n","                response = requests.post(url,json=body, timeout=10)\n","                print(response.status_code)\n","\n","        except Exception as e:\n","                print(\"too many requests/timeout... nap time ! zzzzzz\")\n","                time.sleep(5)\n","                response = requests.post(url,json=body, timeout=10)\n","                print(response.status_code)\n","\n","        try:\n","            data = response.json()\n","            datapoints = data[0][\"object\"][\"vectorDataPoint\"]\n","            \n","            for dp in datapoints:\n","                rows.append({\n","                    \"TRADE_DATE\": dp[\"refPer\"],\n","                    \"REPORTER_KEY\": 1,\n","                    \"PARTNER_KEY\": state,\n","                    \"HS_CODE\": hs,\n","                    \"FLOW_CODE\": 2,\n","                    \"CURRENCY_KEY\": 1,\n","                    \"VALUE\": dp[\"value\"]\n","                })\n","                id += 1\n","\n","            print(id)\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Failed to parse data for STATE {state}, HS {hs}: {e}. skipping :P\")\n","            continue\n","\n","print(\"Done\") \n","\n","schema = StructType([\n","    StructField(\"TRADE_DATE\", StringType(), True),\n","    StructField(\"REPORTER_KEY\", IntegerType(), True),\n","    StructField(\"PARTNER_KEY\", IntegerType(), True),\n","    StructField(\"HS_CODE\", IntegerType(), True),\n","    StructField(\"FLOW_CODE\", IntegerType(), True),\n","    StructField(\"CURRENCY_KEY\", IntegerType(), True),\n","    StructField(\"VALUE\", FloatType(), True)         \n","])\n","\n","df = spark.createDataFrame(rows, schema=schema)\n","df.write.mode(\"append\").saveAsTable(\"FACT_TRADE\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad4d50f0-5a49-494a-9681-ac75a7c0be3c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}